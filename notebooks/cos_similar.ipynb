{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "     return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "     return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          text1     text2     text3\n",
      "text1  1.000000  0.289416  0.350025\n",
      "text2  0.289416  1.000000  0.444271\n",
      "text3  0.350025  0.444271  1.000000\n"
     ]
    }
   ],
   "source": [
    "text1 = 'There is no change in hardness under ambient temperatures.'\n",
    "text2 = 'There is change in hardness upon exposure to the solvent.'\n",
    "text3 = 'There is change in hardness upon high pressure.'\n",
    "\n",
    "test_set = [text1, text2, text3]\n",
    "\n",
    "TfidfVec = TfidfVectorizer(tokenizer = LemNormalize)\n",
    "\n",
    "def cos_similarity(text):\n",
    "    tfidf = TfidfVec.fit_transform(text)\n",
    "    return (tfidf * tfidf.T).toarray()\n",
    "\n",
    "mat = cos_similarity(test_set)\n",
    "#print(mat)\n",
    "\n",
    "df = pd.DataFrame(mat, columns = ['text1', 'text2', 'text3'], index = ['text1', 'text2', 'text3'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import string\n",
    "\n",
    "filepath = \"GS.08.54388_Failure_analysis_of_aramid_reinforced_guide-straps_ex_Brigantine_platform.pdf\"\n",
    "pdfFileObj = open(filepath, 'rb') \n",
    "pdfreader = PyPDF2.PdfFileReader(pdfFileObj) \n",
    "count = pdfreader.numPages\n",
    "#exclude = set(string.punctuation)\n",
    "\n",
    "text = ''\n",
    "for j in range(count):\n",
    "    page = pdfreader.getPage(j)\n",
    "    pp = page.extractText()\n",
    "#    text.append(pp)\n",
    "    text = text + pp\n",
    "#    text = ''.join(ch for ch in text if ch not in exclude) \n",
    "    \n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BG/2005/W301 BRIGANTINE BG - MAIN SPOOLPIECE/KELVAR STRAPS  Table of Contents Section Description Page 1Introduction 12Job Completion Summary 23Anomaly Summaries 34Daily Summaries 44.1 CTS 01 - ROV GVI of Riser 2 4 4.2 CTS 02 - Replacement of Piggyback Straps Riser 2 5 4.3 CTS 03 - GVI of 2004 remedial works at Riser 2 6 4.4 CTS 04 - ROV GVI of Riser 1 9 4.5 CTS 05 - Replacement of piggyback guide straps R1 10 5Video Logs 116Photo Logs 137Anomaly Reports 217.1 CTS 01 - ROV GVI of Riser 2 21 7.2 CTS 04 - ROV GVI of Riser 1 21 8Drawings 229Appendix 239.1 Change of Instruction 23 GS.08.54388 \n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize, keywords\n",
    "#from pprint import pprint #paragraph\n",
    "#from textblob import TextBlob\n",
    "\n",
    "#ABSTRACTIVE: generates entire new summary from the text provided \n",
    "#pprint(summarize(text, word_count = 100))\n",
    "#print(summarize(text, ratio = 0.05))\n",
    "#print(summarize(text, split = True))\n",
    "\n",
    "text_sum = summarize(text, word_count = 100)\n",
    "#unique_sentences = []\n",
    "#for sentence in [sent.raw for sent in TextBlob(text_sum).sentences]:\n",
    "#    if sentence not in unique_sentences:\n",
    "#        unique_sentences.append(sentence)\n",
    "#        text = ' '.join(unique_sentences)\n",
    "\n",
    "print(text_sum)\n",
    "\n",
    "#pprint(keywords(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity to a phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = 'There is change in hardness upon exposure to the solvent. There is no change in hardness under ambient temperatures.'\n",
    "\n",
    "test_set = [text1, text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.18764328]\n",
      " [0.18764328 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "TfidfVec = TfidfVectorizer(tokenizer = LemNormalize)\n",
    "\n",
    "def cos_similarity(text):\n",
    "    tfidf = TfidfVec.fit_transform(text)\n",
    "    return (tfidf * tfidf.T).toarray()\n",
    "\n",
    "print(cos_similarity(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          text1     text2\n",
      "text1  1.000000  0.187643\n",
      "text2  0.187643  1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(cos_similarity(test_set), columns = ['text1', 'text2'], index = ['text1', 'text2'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = cos_similarity(test_set)\n",
    "\n",
    "print(np.triu(a)) #np.tril(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
