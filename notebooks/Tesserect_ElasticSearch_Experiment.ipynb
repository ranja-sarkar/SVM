{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from watchdog.events import PatternMatchingEventHandler\n",
    "from watchdog.observers import Observer\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-Do\n",
    "1. (DONE) Create a bulk OCR and indexing process.\n",
    "    - (DONE) Test if FOR loop is working. FOR loop only goes after pdf files within the same directory level. It doesn't drill to subdirectories.\n",
    "    - (DEBT) Scenario test:\n",
    "        - what if it's OCR is taking too long. Can we batch it up and resume?\n",
    "        - **(NEXT)** what if indexing is too long?\n",
    "               - (DONE) Have to use watcher/watchdog or other tools to monitor the addition of files in the folder.\n",
    "               While the python code is running: \n",
    "               - (DONE) add an additional pdf file into the folder and see if it's added.\n",
    "               -  **modify the filename of the file in the folder, see if only the index is changed.**\n",
    "                   - Attempt to update the \"page\" of multiple pages for the one doc.\n",
    "                   - Use 'GET /test_index_pdf2txt_new/_search' to verify in kibana\n",
    "               - delete the file, see if it removes the index.\n",
    "               - IDEA: Consider having one document to one index. This makes managing indices simpler by design and less memory.\n",
    "               \n",
    "        - (DEBT) Add parallel CPUs for the CV bit of Tesserect.\n",
    "    - (DEBT) Can Elasticsearch Analyzer or kibana Discovery show line breaks \\n\n",
    "        - Ans: the \\n will go in but suspect that the Analyzer removes them\n",
    "    - **(NEXT)** How to create a link in Elasticsearch to open the actual pdf file?\n",
    "        - (NEXT) Use the attachment ingestion instead\n",
    "        - Find a way to add the path and filename into the Kibana.\n",
    "        - Find a way to make the path and filename clickable as a hyperref.\n",
    "        \n",
    "2. Data enrichment\n",
    "    - How can user directly correct the index itself?\n",
    "         - Use can correct the corpus itself and annotate the corpus filename.\n",
    "         - Exception if-else has to be programmed when the original PDF content is changed or if Tesserect is upgraded.\n",
    "         - If we break up the process to CV and corpus indexing, ideally there should be two monitoring at the PDF level and the courpus level.\n",
    "    - How do we tag that this version of the index has been annotated.\n",
    "    \n",
    "3. Find out the OCR performance difference for these two codes:\n",
    "    \n",
    "    a. <code>pages = convert_from_path(pdf_path, 500, first_page=0, last_page=num_pages, poppler_path=r\"poppler-0.68.0_x86/poppler-0.68.0/bin\")</code>\n",
    "    \n",
    "    <code>pages = convert_from_path(pdf_path, poppler_path=r\"poppler-0.68.0_x86/poppler-0.68.0/bin\")</code>\n",
    "    \n",
    "    b. Apache Tika\n",
    "    \n",
    "    c. FSCrawler\n",
    "\n",
    "4. **(NEXT)** How to productionize this in an .exe file?\n",
    " - Build a front end link: https://medium.com/analytics-vidhya/building-a-basic-search-engine-using-elasticsearch-fscrawler-97104c1ea220\n",
    "\n",
    "5. How to build an index pattern from REST without using Kibana?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Below utilizes the code lessons from\n",
    "http://brunorocha.org/python/watching-a-directory-for-file-changes-with-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path C:\\Users\\Tian-Yan.Teh\\Documents\\1 - Projects\\Commercial Text Analytics\\20200922 code\\PDF_Converter\\Convert/\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\Tian-Yan.Teh\\Documents\\1 - Projects\\Commercial Text Analytics\\20200922 code\\PDF_Converter\\Convert/\"\n",
    "print(\"Path\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision (OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************************\n",
    "# Purpose: Converts one pdf into images, then into text by pages.\n",
    "# Input: Path of one pdf file\n",
    "# Output: A dictionary (docs) containing page and text (line break \\n included)\n",
    "#************************************************************\n",
    "def pdf2txt(pdf_path):    \n",
    "\n",
    "    d = {}\n",
    "    docs = [] #Empty list that will carry the \"page\" and \"text\" field. \n",
    "    \n",
    "    pytesseract.pytesseract.tesseract_cmd = r'tesseract\\tesseract.exe'\n",
    "    \n",
    "    images = convert_from_path(pdf_path, poppler_path=r\"poppler-0.68.0_x86/poppler-0.68.0/bin\")\n",
    "    count = 0\n",
    "    for image in images:\n",
    "        output = pytesseract.image_to_string(image)\n",
    "\n",
    "        # TO-DO: Take only a filename instead of the fullpath\n",
    "        d['page'] = pdf_path[:-4]+'_Page_'+str((count+1))\n",
    "        d['text'] = output\n",
    "        docs.append(d.copy()) #docs = list of dictionary items.\n",
    "        count += 1\n",
    "        \n",
    "    print(\"END\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************************\n",
    "# Purpose: Bulk (batch) converts all pdfs in a folder into images, then into text by pages.\n",
    "# Input: Path of pdf files\n",
    "# Output: A dictionary (docs) containing page and text (line break \\n included)\n",
    "#************************************************************\n",
    "# def pdf2txtbatch(path):\n",
    "    \n",
    "#     #files = List of all filenames.\n",
    "#     files = os.listdir(path)\n",
    "#     d = {} #why is it in an dictionary?\n",
    "#     docs = []\n",
    "    \n",
    "#     #files_pdf = List of all filenames of PDFs files.\n",
    "#     files_pdf = [f for f in files if f[-3:] == 'pdf'] \n",
    "    \n",
    "#     pytesseract.pytesseract.tesseract_cmd = r'tesseract\\tesseract.exe'\n",
    "    \n",
    "#     #files_pdf = List of all filenames of PDFs files.\n",
    "#     #file_pdf = file names of each PDF.\n",
    "#     for file_pdf in files_pdf:\n",
    "#         #Render pdfs to images using Poppler\n",
    "#         #images = list of document images/pages\n",
    "#         images = convert_from_path(file_pdf, poppler_path = r\"poppler-0.68.0_x86/poppler-0.68.0/bin\")\n",
    "#         count = 0\n",
    "        \n",
    "#         for image in images:\n",
    "#             output = pytesseract.image_to_string(image)\n",
    "            \n",
    "#             d['page'] = file_pdf[:-4]+'_Page_'+str((count+1))\n",
    "#             d['text'] = output\n",
    "#             docs.append(d.copy()) #docs = list of dictionary items.\n",
    "#             count += 1\n",
    "            \n",
    "#     print(\"END\")\n",
    "#     return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es = Elasticsearch(['localhost:9200'])\n",
    "\n",
    "# # TO-DO: To make this into a class.\n",
    "# # Input: doc-ID, new file name.\n",
    "# # Output: renamed method. \n",
    "\n",
    "# #************************************************************\n",
    "# # client.update()\n",
    "# #************************************************************\n",
    "# # Purpose: Update an Elasticsearch Index\n",
    "# # Link: https://kb.objectrocket.com/mongo-db/how-to-use-python-to-update-api-elasticsearch-documents-259\n",
    "# # Input: ?\n",
    "# # Output: ?\n",
    "# #************************************************************\n",
    "\n",
    "# source_to_update = {\n",
    "#     # \"\"doc\"\" is essentially Elasticsearch's \"\"_source\"\" field\n",
    "#     \"doc\" : {\n",
    "#         \"page\" : r\"C:\\Users\\Tian-Yan.Teh\\Documents\\1 - Projects\\Commercial Text Analytics\\20200922 code\\PDF_Converter\\Convert\\Renamed_Page_1\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# response = es.update(index='test_index_pdf2txt_new', doc_type='_doc', id='uq63TncBjQde-dUNKnS_', body=source_to_update)\n",
    "# print ('response:', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = requests.get('http://localhost:9200/')\n",
    "# print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xget = requests.get('http://localhost:9200/_cat/indices?v&pretty')\n",
    "# print(xget.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a use case for 1. modified, 2. created, 3. deleted, 4. moved\n",
    "class MyHandler(PatternMatchingEventHandler):\n",
    "    patterns=[\"*.pdf\"]\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def process(self, event):\n",
    "        \"\"\"\n",
    "        event.event_type\n",
    "            'modified' | 'created' | 'moved' | 'deleted'\n",
    "        event.is_directory\n",
    "            True | False\n",
    "        event.src_path\n",
    "            path/to/observed/file\n",
    "        \"\"\"\n",
    "        \n",
    "        # the file will be processed there\n",
    "        print(event.src_path, event.event_type, type(event.event_type))  # print now only for debugging\n",
    "        \n",
    "        # Start the ElasticSearch client\n",
    "        es = Elasticsearch(['localhost:9200'])\n",
    "        \n",
    "        if event.event_type == 'created':\n",
    "            \n",
    "            #Pseudocode:  Pasting new pdfs -> perform OCR -> convert to corpus -> index it\n",
    "            # write a file_path one-by-one and index one by one without deleting the index.\n",
    "            docs = pdf2txt(event.src_path)\n",
    "            index = \"test_index\"\n",
    "\n",
    "            ## elastic helper function to bulk index json\n",
    "            bulk(es, docs, index=index, doc_type='_doc', raise_on_error=True)\n",
    "            print(\"Created\")\n",
    "            \n",
    "        elif event.event_type == 'deleted':\n",
    "            print(\"Deleted\")\n",
    "            # Deleting existing pdfs / cut and paste elsewhere -> remove index\n",
    "            # TO-DO: How do I get it to only delete the doc and not the entire index.\n",
    "            \n",
    "            #Code: to delete the entire index.\n",
    "            #es.indices.delete(index_name)\n",
    "            \n",
    "        elif event.event_type == 'moved':\n",
    "            print(\"Renamed\")\n",
    "            # Renamed the pdf -> modify index [old name] -> rename index to [new name]\n",
    "\n",
    "            # if event.event_type == 'moved': [old name]\n",
    "            # Retrieve for list of ids with that name.\n",
    "            # if event.event_type == 'modified': [new name]\n",
    "            # Bulk update for all those names with this id.\n",
    "            \n",
    "#             source_to_update = {\n",
    "#                 # \"\"doc\"\" is essentially Elasticsearch's \"\"_source\"\" field\n",
    "#                 \"doc\" : {\n",
    "#                     \"page\" : r\"C:\\Users\\Tian-Yan.Teh\\Documents\\1 - Projects\\Commercial Text Analytics\\20200922 code\\PDF_Converter\\Convert\\Renamed_Again_Page_1\"\n",
    "#                 }\n",
    "#             }\n",
    "\n",
    "#             # TO-DO: Un-hardcode the id.\n",
    "#             response = es.update(index='test_index', doc_type='_doc', id='uq63TncBjQde-dUNKnS_', body=source_to_update)\n",
    "#             print ('response:', response)\n",
    "            \n",
    "        else:\n",
    "            exit()\n",
    "    \n",
    "    def on_created(self, event): #Executed when a file or a directory is created\n",
    "        self.process(event)\n",
    "           \n",
    "    def on_deleted(self, event): #Executed when a file or directory is deleted.\n",
    "         self.process(event)\n",
    "    \n",
    "    def on_modified(self, event): #Executed when a file is modified or a directory renamed\n",
    "         self.process(event)\n",
    "        \n",
    "    def on_moved(self, event): #Executed when a file or directory is moved\n",
    "         self.process(event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirname C:\\Users\\Tian-Yan.Teh\\Documents\\1 - Projects\\Commercial Text Analytics\\20200922 code\\PDF_Converter\\Convert\n",
      "C:\\Users\\Tian-Yan.Teh\\Documents\\1 - Projects\\Commercial Text Analytics\\20200922 code\\PDF_Converter\\Convert\\28072020_MANUAL_PENGGUNA_(KIOSK_DAN_SISTEM_SMARTBOX)(1 min).pdf deleted <class 'str'>\n",
      "Deleted\n",
      "C:\\Users\\Tian-Yan.Teh\\Documents\\1 - Projects\\Commercial Text Analytics\\20200922 code\\PDF_Converter\\Convert\\Shell Overview of Upstream Training (4 sec).pdf deleted <class 'str'>\n",
      "Deleted\n",
      "C:\\Users\\Tian-Yan.Teh\\Documents\\1 - Projects\\Commercial Text Analytics\\20200922 code\\PDF_Converter\\Convert\\CA-dated 24.9.2003.pdf created <class 'str'>\n",
      "END\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tian-Yan.Teh\\Anaconda3\\lib\\site-packages\\elasticsearch\\connection\\base.py:190: ElasticsearchDeprecationWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created\n",
      "C:\\Users\\Tian-Yan.Teh\\Documents\\1 - Projects\\Commercial Text Analytics\\20200922 code\\PDF_Converter\\Convert\\CA-dated 24.9.2003.pdf modified <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#************************************************************\n",
    "#dirname\n",
    "#************************************************************\n",
    "# Commented for use only when a .py file is created.\n",
    "#if __name__ == '__main__':\n",
    "# args = sys.argv[1:]\n",
    "# print(\"args:\",args)\n",
    "#\n",
    "observer = Observer()\n",
    "# observer.schedule(MyHandler(), path=args[0] if args else '.')\n",
    "#************************************************************\n",
    "\n",
    "#************************************************************\n",
    "#dirname\n",
    "#************************************************************\n",
    "#*     The following hardcoing is necessary only for testing in .ipynb\n",
    "dirname=os.path.dirname(path)\n",
    "#*     Commented out because __file__ only works in a .py\n",
    "#dirname=os.path.dirname(os.path.abspath(__file__))\n",
    "#************************************************************\n",
    "\n",
    "print(\"Dirname\", dirname)\n",
    "observer.schedule(MyHandler(), dirname) #Set recursive=True if we want to go to the subdirectories too\n",
    "observer.start()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    observer.stop()\n",
    "\n",
    "observer.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue\n",
    "\n",
    "Code: bulk(es, docs, index=index, doc_type='_doc', raise_on_error=True)\n",
    "Error message: raise BulkIndexError(\"%i document(s) failed to index.\" % len(errors), errors)\n",
    "elasticsearch.helpers.errors.BulkIndexError: ('52 document(s) failed to index.', [{'index': {'_index': 'test_index_pdf2txt', '_type': '_doc', '_id': 'Ba6tTXcBjQde-dUNTlsH', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': \"failed to parse field [page] of type [long] in document with id 'Ba6tTXcBjQde-dUNTlsH'. Preview of field's value: 'C:\\\\Users\\\\Tian-Yan.Teh\\\\Documents\\\\1 - Projects\\\\Commercial Text Analytics\\\\20200922 code\\\\PDF_Converter\\\\Convert\\\\GSA dated 22.7.92_Page_1'\", 'caused_by': {'type': 'illegal_argument_exception', 'reason': 'For input string: \"C:\\\\Users\\\\Tian-Yan.Teh\\\\Documents\\\\1 - Projects\\\\Commercial Text Analytics\\\\20200922 code\\\\PDF_Converter\\\\Convert\\\\GSA dated 22.7.92_Page_1\"'}}, 'data': {'page': 'C:\\\\Users\\\\Tian-Yan.Teh\\\\Documents\\\\1 - Projects\\\\Commercial Text Analytics\\\\20200922 code\\\\PDF_Converter\\\\Convert\\\\GSA dated 22.7.92_Page_1', 'text': 'GAS SALES AGREEMENT\\n\\nBETWEEN\\n\\nPETROLIAM NASIONAL BERHAD\\n\\nAND\\n\\nSARAWAK SHELL BERHAD\\n\\nFOR\\n\\nSUPPLY OF GAS TO LUTONG REFINERY\\n\\nwe\\n\\x0c'}}}, \n",
    "\n",
    "Findings: Used a different index. Not sure how this problem was solved in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. Watching a directory for file changes with Python\n",
    "http://brunorocha.org/python/watching-a-directory-for-file-changes-with-python.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Unused coding\n",
    "## Single pdf converter and indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************************\n",
    "# Purpose: Bulk converts all pdf in a folder into images, then into text by pages.\n",
    "# Input: Path of pdf files\n",
    "# Output: A dictionary (docs) containing page and text (line break \\n included)\n",
    "#************************************************************\n",
    "def pdf2txtbatch(folder_path):\n",
    "    \n",
    "    #files = List of all filenames.\n",
    "    files = os.listdir(folder_path)\n",
    "    d = {} #Dictionary. Why this data structure?\n",
    "    docs = []\n",
    "    #files_pdf = List of all filenames of PDFs files.\n",
    "    files_pdf = [f for f in files if f[-3:] == 'pdf'] \n",
    "    \n",
    "    pytesseract.pytesseract.tesseract_cmd = r'tesseract\\tesseract.exe'\n",
    "    \n",
    "    #files_pdf = List of all filenames of PDFs files.\n",
    "    #file_pdf = file names of each PDF.\n",
    "    for file_pdf in files_pdf:\n",
    "        #Render pdfs to images using Poppler\n",
    "        #images = list of document images/pages\n",
    "        images = convert_from_path(file_pdf, poppler_path = r\"poppler-0.68.0_x86/poppler-0.68.0/bin\")\n",
    "        count = 0\n",
    "        \n",
    "        for image in images:\n",
    "            output = pytesseract.image_to_string(image)\n",
    "            \n",
    "            d['page'] = file_pdf[:-4]+'_Page_'+str((count+1))\n",
    "            d['text'] = output\n",
    "            docs.append(d.copy()) #docs = list of dictionary items.\n",
    "            count += 1\n",
    "            \n",
    "    print(\"END\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sandbox experiment\n",
    "pdf_path = \"test.pdf\"\n",
    "pdf2txt(pdf_path, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "\n",
    "es = Elasticsearch(['localhost:9200'])\n",
    "#docs = tokens / corpus???\n",
    "docs = pdf2txt(pdf_path)\n",
    "index = \"test_index_pdf2txt\"\n",
    "\n",
    "##good practice to delete an index if it already exists and you're overwriting\n",
    "if es.indices.exists(index):\n",
    "    es.indices.delete(index)\n",
    "\n",
    "## elastic helper function to bulk index json\n",
    "bulk(es, docs, index=index, doc_type='_doc', raise_on_error=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing into Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pdf2txtbatch()\n",
    "index = \"test_index_pdf2txtbatch\"\n",
    "\n",
    "##good practice to delete an index if it already exists and you're overwriting\n",
    "if es.indices.exists(index):\n",
    "    es.indices.delete(index)\n",
    "\n",
    "## elastic helper function to bulk index json\n",
    "bulk(es, docs, index=index, doc_type='_doc', raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
