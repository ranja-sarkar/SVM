{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import pandas as pd\r\n",
    "import numpy as np \r\n",
    "import PyPDF2\r\n",
    "import string\r\n",
    "import nltk\r\n",
    "from nltk.tokenize.regexp import regexp_tokenize\r\n",
    "import sumy\r\n",
    "from sumy.parsers.plaintext import PlaintextParser\r\n",
    "from sumy.nlp.tokenizers import Tokenizer\r\n",
    "from sumy.summarizers.lsa import LsaSummarizer\r\n",
    "import re\r\n",
    "from nltk.util import ngrams\r\n",
    "import math\r\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "path = \"r_s_test\"\r\n",
    "docs = os.listdir(path)\r\n",
    "n = len(docs)\r\n",
    "\r\n",
    "exclude = set(string.punctuation)\r\n",
    "docslist = []\r\n",
    "alldocslist = []\r\n",
    "for index, i in  enumerate(docs):\r\n",
    "    doctext = []\r\n",
    "    if (i.endswith(\".pdf\")) or (i.endswith(\".PDF\")):\r\n",
    "        in_file = os.path.abspath(path + \"\\\\\" + i)\r\n",
    "        in_file1 = in_file.encode('UTF-8')\r\n",
    "        pdfileobj = open(in_file1,'rb')\r\n",
    "        pdfreader = PyPDF2.PdfFileReader(pdfileobj)   \r\n",
    "        count = pdfreader.numPages\r\n",
    "        for j in range(count):\r\n",
    "            page = pdfreader.getPage(j)\r\n",
    "            pp = page.extractText()\r\n",
    "            doctext.append(pp)\r\n",
    "            \r\n",
    "        dt = ''.join(ch for ch in doctext)\r\n",
    "        docslist.append(dt)\r\n",
    "        doctext = ''.join(ch for ch in doctext if ch not in exclude)        \r\n",
    "        alldocslist.append(doctext)  \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "plot_data1 = [[]] * len(alldocslist)\r\n",
    "plot_data2 = [[]] * len(alldocslist)\r\n",
    "plot_data3 = [[]] * len(alldocslist)\r\n",
    "\r\n",
    "for doc in alldocslist:\r\n",
    "    doctext = doc\r\n",
    "    tokentext = regexp_tokenize(doctext, pattern = '\\s+', gaps = True)\r\n",
    "    plot_data1[index].append(tokentext)\r\n",
    "    plot_data2[index].append(tokentext)\r\n",
    "    plot_data3[index].append(tokentext)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def text_summarizer(docx, sentN):\r\n",
    "    #docx = re.sub(r\"[0-9]+\", 'x', docx)\r\n",
    "    parser = PlaintextParser.from_string(docx, Tokenizer(language = \"english\"))\r\n",
    "    summarizer_lsa = LsaSummarizer()\r\n",
    "    summary = summarizer_lsa(parser.document, sentN)\r\n",
    "#    for sentences in summary:\r\n",
    "#        print(sentences)\r\n",
    "   # textsumm = ' '.join(str(sentences) for sentences in summary) + '.'\r\n",
    "    textsumm = ' '.join(str(sentences) for sentences in summary)\r\n",
    "    return textsumm\r\n",
    "\r\n",
    "text_data = [[]] * len(docslist)\r\n",
    "for doc in docslist:\r\n",
    "    doctext = doc\r\n",
    "    txtsum = text_summarizer(doctext, 2)\r\n",
    "    text_data[index].append(txtsum)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "for x in range(len(docs)):  \r\n",
    "    unigrams = list(ngrams(plot_data1[0][x], 1))\r\n",
    "    grams1 = [' '.join(unigrams[n]) for n in range(len(unigrams))]\r\n",
    "    \r\n",
    "    bigrams = list(ngrams(plot_data1[0][x], 2))\r\n",
    "    grams2 = [' '.join(bigrams[n]) for n in range(len(bigrams))]\r\n",
    "    \r\n",
    "    trigrams = list(ngrams(plot_data1[0][x], 3))\r\n",
    "    grams3 = [' '.join(trigrams[n]) for n in range(len(trigrams))]\r\n",
    "    \r\n",
    "    plot_data1[0][x] = grams1 \r\n",
    "    plot_data2[0][x] = grams2\r\n",
    "    plot_data3[0][x] = grams3 \r\n",
    "\r\n",
    "\r\n",
    "l1 = plot_data1[0]\r\n",
    "l2 = plot_data2[0]\r\n",
    "l3 = plot_data3[0]\r\n",
    "\r\n",
    "flatten1 = [item for sublist in l1 for item in sublist]\r\n",
    "flatten2 = [item for sublist in l2 for item in sublist]\r\n",
    "flatten3 = [item for sublist in l3 for item in sublist]\r\n",
    "words1 = flatten1\r\n",
    "words2 = flatten2\r\n",
    "words3 = flatten3\r\n",
    "\r\n",
    "wordsunique1 = set(words1) \r\n",
    "wordsunique1 = list(wordsunique1)\r\n",
    "wordsunique2 = set(words2) \r\n",
    "wordsunique2 = list(wordsunique2)\r\n",
    "wordsunique3 = set(words3) \r\n",
    "wordsunique3 = list(wordsunique3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def tf(word, doc):\r\n",
    "    return doc.count(word)/len(doc)\r\n",
    "\r\n",
    "def containing(word, doclist):\r\n",
    "    return sum(1 for doc in doclist if word in doc)\r\n",
    "\r\n",
    "def idf(word, doclist):\r\n",
    "    return math.log(len(doclist)/(containing(word, doclist)))\r\n",
    "\r\n",
    "def tfidf(word, doc, doclist):\r\n",
    "    counter = (tf(word, doc)*idf(word, doclist)) * 100\r\n",
    "    return (round(counter, 2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "plottest = text_data[0][0:n]\r\n",
    "\r\n",
    "#UNIGRAMS\r\n",
    "plottest1 = plot_data1[0][0:n]\r\n",
    "#t0 = round((time.clock())/60, 2)\r\n",
    "search_word = {}\r\n",
    "for doc in plottest1:\r\n",
    "    for word in wordsunique1:\r\n",
    "        if word in doc:\r\n",
    "            word = str(word)\r\n",
    "            index = plottest1.index(doc)\r\n",
    "            doc_name = docs[index]\r\n",
    "            idfs = tfidf(word, doc, plottest1)\r\n",
    "            try:\r\n",
    "                search_word[word].append([doc_name, plottest[index], idfs])\r\n",
    "            except:\r\n",
    "                search_word[word] = []\r\n",
    "                search_word[word].append([doc_name, plottest[index], idfs])\r\n",
    "\r\n",
    "\r\n",
    "#BIGRAMS\r\n",
    "plottest2 = plot_data2[0][0:n]\r\n",
    "#plottest = text_data[0][0:n]\r\n",
    "#t0 = round((time.clock())/60, 2)\r\n",
    "search_word1 = {}\r\n",
    "for doc in plottest2:\r\n",
    "    for word in wordsunique2:\r\n",
    "        if word in doc:\r\n",
    "            word = str(word)\r\n",
    "            index = plottest2.index(doc)\r\n",
    "            doc_name = docs[index]\r\n",
    "            idfs = tfidf(word, doc, plottest2)\r\n",
    "            try:\r\n",
    "                search_word1[word].append([doc_name, plottest[index], idfs])\r\n",
    "            except:\r\n",
    "                search_word1[word] = []\r\n",
    "                search_word1[word].append([doc_name, plottest[index], idfs])\r\n",
    "\r\n",
    "\r\n",
    "search_word.update(search_word1)\r\n",
    "\r\n",
    "#TRIGRAMS\r\n",
    "plottest3 = plot_data3[0][0:n]\r\n",
    "#plottest = text_data[0][0:n]\r\n",
    "#t0 = round((time.clock())/60, 2)\r\n",
    "search_word2 = {}\r\n",
    "for doc in plottest3:\r\n",
    "    for word in wordsunique3:\r\n",
    "        if word in doc:\r\n",
    "            word = str(word)\r\n",
    "            index = plottest3.index(doc)\r\n",
    "            doc_name = docs[index]\r\n",
    "            idfs = tfidf(word, doc, plottest3)\r\n",
    "            try:\r\n",
    "                search_word2[word].append([doc_name, plottest[index], idfs])\r\n",
    "            except:\r\n",
    "                search_word2[word] = []\r\n",
    "                search_word2[word].append([doc_name, plottest[index], idfs])\r\n",
    "\r\n",
    "\r\n",
    "search_word.update(search_word2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "filename = \"test.pkl\"\r\n",
    "fileobj = open(filename,'wb')\r\n",
    "pickle.dump(search_word, fileobj)\r\n",
    "fileobj.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "af8b93295ec152667600696eebafdfb1a6679e79e15871d01906b24cd0cc4b31"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}